---
phase: "04-advanced-quality"
plan: "03"
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - "src/subtext/types.ts"
  - "src/subtext/subtext-engine.ts"
  - "src/subtext/subtext-prompts.ts"
  - "src/subtext/index.ts"
  - "src/voice/types.ts"
  - "src/voice/character-voice.ts"
  - "src/voice/voice-metrics.ts"
  - "src/voice/voice-prompts.ts"
  - "src/voice/index.ts"
  - "schemas/subtext-annotation.schema.json"
  - "schemas/voice-profile.schema.json"
  - "src/quality/stage-evaluators.ts"
  - "tests/subtext/subtext-engine.test.ts"
  - "tests/voice/character-voice.test.ts"
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Dialogue scenes contain observable emotional subtext (characters say one thing, mean another)"
    - "Hidden emotion layers are annotated with surface meaning, actual intention, underlying emotion"
    - "Physical manifestations (body language, action beats, vocal cues) accompany subtext"
    - "Each character has a distinct voice pattern (vocabulary, sentence structure, speech habits)"
    - "Character voices remain consistent across chapters (voice drift is detected)"
  artifacts:
    - path: "src/subtext/types.ts"
      provides: "SubtextAnnotation, EmotionLayer interfaces"
      exports: ["SubtextAnnotation", "EmotionLayer", "PhysicalManifestations"]
    - path: "src/subtext/subtext-engine.ts"
      provides: "Dialogue emotion layer annotation"
      exports: ["annotateDialogueSubtext", "buildSubtextPrompt"]
    - path: "src/voice/types.ts"
      provides: "VoiceProfile, VoiceFingerprint interfaces"
      exports: ["VoiceProfile", "VoiceFingerprint", "SpeechPatterns"]
    - path: "src/voice/character-voice.ts"
      provides: "Per-character voice profile management"
      exports: ["createVoiceProfile", "updateVoiceProfile"]
    - path: "src/voice/voice-metrics.ts"
      provides: "Voice consistency measurement"
      exports: ["analyzeVoiceConsistency", "computeVoiceFingerprint"]
    - path: "schemas/subtext-annotation.schema.json"
      provides: "JSON Schema for SubtextAnnotation"
      contains: "subtextLayers"
    - path: "schemas/voice-profile.schema.json"
      provides: "JSON Schema for VoiceProfile"
      contains: "speechPatterns"
  key_links:
    - from: "src/subtext/subtext-engine.ts"
      to: "src/quality/stage-evaluators.ts"
      via: "ToneStageEvaluator subtext detection"
      pattern: "import.*annotateDialogueSubtext|SubtextAnnotation"
    - from: "src/voice/voice-metrics.ts"
      to: "src/quality/stage-evaluators.ts"
      via: "ToneStageEvaluator voice consistency"
      pattern: "import.*analyzeVoiceConsistency|VoiceProfile"
---

<objective>
Create the emotional subtext engine and character voice differentiation systems that add psychological depth to dialogue and ensure each character speaks distinctly.

Purpose: Address the "characters say exactly what they mean" AI writing failure mode by adding hidden emotion layers. Ensure each character has a recognizable voice pattern that remains consistent across chapters.

Output: Two new modules (src/subtext/, src/voice/) with annotation schemas, analysis engines, and integration with the Tone stage evaluator.
</objective>

<execution_context>
@C:\Users\jodnr\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jodnr\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-advanced-quality/04-RESEARCH.md
@.planning/phases/04-advanced-quality/04-01-PLAN.md

# Quality module from Plan 01
@src/quality/types.ts
@src/quality/stage-evaluators.ts
@src/pipeline/types.ts
@src/korean/honorific-matrix.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create subtext module with annotation schema and engine</name>
  <files>
    src/subtext/types.ts
    src/subtext/subtext-engine.ts
    src/subtext/subtext-prompts.ts
    src/subtext/index.ts
    schemas/subtext-annotation.schema.json
  </files>
  <action>
**src/subtext/types.ts:**
```typescript
export interface EmotionLayer {
  level: number;                       // 1 = primary, 2 = deeper, 3 = deepest
  actualIntention: string;             // What character is really trying to do
  underlyingEmotion: string;           // The emotion beneath
  hiddenContext: string;               // What they're NOT saying
  tellSigns: string[];                 // How this manifests subtly
}

export interface PhysicalManifestations {
  bodyLanguage: string[];              // "clenched jaw", "avoided eye contact"
  actionBeats: string[];               // "fidgeted with ring", "turned away"
  vocalCues: string[];                 // "voice dropped", "words came too fast"
}

export interface SubtextAnnotation {
  dialogueId: string;                  // Reference to dialogue in scene
  speakerId: string;
  listenerId: string;
  surfaceLevel: {
    text: string;                      // What character literally says
    topic: string;                     // Apparent subject matter
  };
  subtextLayers: EmotionLayer[];       // 1-3 layers of hidden meaning
  physicalManifestations: PhysicalManifestations;
  narrativeFunction: 'reveal' | 'conceal' | 'deflect' | 'test' | 'manipulate';
}

export interface SubtextContext {
  scene: { id: string; emotionalArc?: string };
  characters: Array<{ id: string; name: string; personality?: string; currentEmotionalState?: string }>;
  relationshipDynamics: Array<{ char1: string; char2: string; description: string }>;
  sceneGoals: string[];
  emotionalStakes: string;
}
```

**src/subtext/subtext-prompts.ts:**
Create buildSubtextPrompt(dialogue, speaker, listener, relationship, context) that generates LLM prompt for subtext analysis.
Prompt should ask:
1. What is the speaker ACTUALLY trying to communicate?
2. What emotion are they hiding or suppressing?
3. What are they deliberately NOT saying?
4. How would this manifest physically?

**src/subtext/subtext-engine.ts:**
- `annotateDialogueSubtext(dialogue, context): Promise<SubtextAnnotation>` - Main annotation function
- `detectFlatDialogue(content, existingAnnotations): Array<{location, currentText, suggestedSubtext}>` - Find dialogue lacking subtext
- `shouldHaveSubtext(dialogueText, context): boolean` - Heuristic for which dialogues need subtext (emotionally significant moments)

**src/subtext/index.ts:**
Export all types and functions.

**schemas/subtext-annotation.schema.json:**
JSON Schema validating SubtextAnnotation with required fields: dialogueId, speakerId, listenerId, surfaceLevel, subtextLayers (1-3 items), physicalManifestations, narrativeFunction.
  </action>
  <verify>
npx tsc --noEmit
node -e "const s = require('./src/subtext/index.js'); console.log('Subtext exports:', Object.keys(s));"
  </verify>
  <done>
- src/subtext/ module exists with types, engine, prompts, index
- SubtextAnnotation captures surface text, emotion layers, physical manifestations
- buildSubtextPrompt generates appropriate LLM prompt
- JSON Schema validates SubtextAnnotation structure
  </done>
</task>

<task type="auto">
  <name>Task 2: Create voice module with profile schema and consistency metrics</name>
  <files>
    src/voice/types.ts
    src/voice/character-voice.ts
    src/voice/voice-metrics.ts
    src/voice/voice-prompts.ts
    src/voice/index.ts
    schemas/voice-profile.schema.json
  </files>
  <action>
**src/voice/types.ts:**
```typescript
export interface SpeechPatterns {
  sentenceStructure: {
    preferredLength: 'short' | 'medium' | 'long' | 'varied';
    complexityLevel: 'simple' | 'moderate' | 'complex';
    fragmentUsage: 'never' | 'rare' | 'occasional' | 'frequent';
  };
  vocabulary: {
    register: 'colloquial' | 'standard' | 'formal' | 'literary';
    technicalTerms?: string[];
    avoidedWords?: string[];
    favoredExpressions: string[];
  };
  verbalHabits: {
    fillers: string[];                 // "um", "like", "저기"
    catchphrases: string[];            // Signature expressions
    exclamations: string[];            // "Oh my!", "이런!"
    hedging: 'none' | 'minimal' | 'moderate' | 'heavy';
  };
  rhythm: {
    tempo: 'slow' | 'moderate' | 'fast' | 'variable';
    pauseUsage: 'rare' | 'occasional' | 'frequent';
    interruption: 'never' | 'sometimes' | 'often';
  };
}

export interface InternalMonologue {
  style: 'stream-of-consciousness' | 'analytical' | 'emotional' | 'sparse';
  selfAddressing: 'first-person' | 'second-person' | 'none';
  tangentFrequency: 'low' | 'medium' | 'high';
}

export interface VoiceFingerprint {
  avgSentenceLength: number;
  vocabularyComplexity: number;        // 0-1 scale
  dialogueToNarrationRatio: number;
  exclamationFrequency: number;        // per 1000 chars
  questionFrequency: number;           // per 1000 chars
  fillerWordDensity: number;           // per 1000 chars
}

export interface VoiceProfile {
  characterId: string;
  characterName: string;
  speechPatterns: SpeechPatterns;
  internalMonologue: InternalMonologue;
  linguisticMarkers: {
    honorificDefault: 'haeche' | 'haeyoche' | 'hapsyoche';
    dialectFeatures?: string[];
    educationSignals: string[];
  };
  voiceFingerprint: VoiceFingerprint;
}

export interface VoiceDeviation {
  location: { paragraphStart: number; paragraphEnd: number };
  aspect: 'vocabulary' | 'structure' | 'habit' | 'rhythm' | 'monologue';
  expected: string;
  found: string;
  severity: 'minor' | 'moderate' | 'major';
}

export interface VoiceConsistencyResult {
  characterId: string;
  overallScore: number;                // 0-100
  deviations: VoiceDeviation[];
  recommendations: string[];
}
```

**src/voice/character-voice.ts:**
- `createVoiceProfile(characterId, name, initialDescription): VoiceProfile` - Initialize profile from character description
- `updateVoiceProfile(profile, dialogueSamples): VoiceProfile` - Refine profile from actual dialogue
- `buildVoiceConstraintPrompt(profile): string` - Generate prompt constraints for generation

**src/voice/voice-metrics.ts:**
- `computeVoiceFingerprint(dialogueSamples): VoiceFingerprint` - Extract quantitative metrics
- `analyzeVoiceConsistency(content, characterId, profile, dialogueAttributions): VoiceConsistencyResult` - Check consistency
- `checkVocabularyConsistency(text, vocabProfile): VoiceDeviation[]` - Check vocabulary matches
- `checkVerbalHabits(text, habits): VoiceDeviation[]` - Check habit usage
- `checkSentenceStructure(text, structureProfile): VoiceDeviation[]` - Check structure matches

**src/voice/voice-prompts.ts:**
- `buildVoiceAnalysisPrompt(dialogueSamples, characterInfo): string` - Prompt for extracting voice patterns
- `buildVoiceGenerationPrompt(profile): string` - Prompt for generating dialogue in character voice

**schemas/voice-profile.schema.json:**
JSON Schema validating VoiceProfile with required: characterId, characterName, speechPatterns, internalMonologue, linguisticMarkers, voiceFingerprint.
  </action>
  <verify>
npx tsc --noEmit
node -e "const v = require('./src/voice/index.js'); console.log('Voice exports:', Object.keys(v));"
  </verify>
  <done>
- src/voice/ module exists with types, character-voice, voice-metrics, voice-prompts, index
- VoiceProfile captures speech patterns, verbal habits, rhythm, fingerprint
- analyzeVoiceConsistency returns score and deviations
- JSON Schema validates VoiceProfile structure
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate subtext and voice with Tone stage evaluator and create tests</name>
  <files>
    src/quality/stage-evaluators.ts
    tests/subtext/subtext-engine.test.ts
    tests/voice/character-voice.test.ts
  </files>
  <action>
**Update src/quality/stage-evaluators.ts:**

Modify ToneStageEvaluator to:
1. Import SubtextAnnotation, detectFlatDialogue from subtext module
2. Import VoiceProfile, analyzeVoiceConsistency from voice module
3. In score():
   - If subtextAnnotations provided in options, check subtext coverage (target 30-40% of dialogue)
   - If voiceProfiles provided in options, check voice consistency per character
   - Combine emotional arc score + subtext coverage + voice consistency
4. In generateDirectives():
   - Call detectFlatDialogue() to find dialogue needing subtext
   - Call analyzeVoiceConsistency() to find voice drift
   - Generate 'dialogue-subtext' directives for flat dialogue (use subtext-injection directive type)
   - Generate 'voice-consistency' directives for voice drift (use voice-drift directive type)

**tests/subtext/subtext-engine.test.ts:**
- Test SubtextAnnotation structure validates correctly
- Test buildSubtextPrompt generates appropriate prompt
- Test shouldHaveSubtext identifies emotionally significant dialogue
- Test detectFlatDialogue finds on-the-nose dialogue
- Test with Korean dialogue samples

**tests/voice/character-voice.test.ts:**
- Test createVoiceProfile initializes from description
- Test computeVoiceFingerprint extracts metrics
- Test analyzeVoiceConsistency detects vocabulary drift
- Test analyzeVoiceConsistency detects structure drift
- Test buildVoiceConstraintPrompt generates readable constraints
- Test with multiple characters having distinct profiles
- Verify characters with same profile score 100
- Verify mismatched dialogue scores lower
  </action>
  <verify>
npx tsc --noEmit
npm test -- tests/subtext/ tests/voice/
  </verify>
  <done>
- ToneStageEvaluator integrates subtext and voice checking
- detectFlatDialogue and analyzeVoiceConsistency called during evaluation
- 'subtext-injection' and 'voice-drift' directives generated appropriately
- tests/subtext/ has 10+ test cases
- tests/voice/ has 10+ test cases
- All tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# TypeScript compilation
npx tsc --noEmit

# Run all Phase 4 tests
npm test -- tests/quality/ tests/subtext/ tests/voice/ tests/style-library/

# Verify module exports
node -e "
const subtext = require('./src/subtext/index.js');
const voice = require('./src/voice/index.js');
console.log('Subtext exports:', Object.keys(subtext));
console.log('Voice exports:', Object.keys(voice));
"

# Test subtext annotation
node -e "
const { buildSubtextPrompt } = require('./src/subtext/index.js');
const prompt = buildSubtextPrompt(
  { text: '괜찮아요. 신경 쓰지 마세요.' },
  { name: '민지', personality: '내성적' },
  { name: '준호' },
  { description: '연인 사이' },
  { sceneGoals: ['갈등 해소'], emotionalStakes: '관계 위기' }
);
console.log('Subtext prompt length:', prompt.length);
"

# Test voice profile
node -e "
const { createVoiceProfile, buildVoiceConstraintPrompt } = require('./src/voice/index.js');
const profile = createVoiceProfile('char_001', '김철수', '격식을 차리는 40대 남성');
console.log('Voice profile created:', profile.characterId);
console.log('Constraint preview:', buildVoiceConstraintPrompt(profile).slice(0, 200));
"
```
</verification>

<success_criteria>
1. src/subtext/ module captures emotion layers and physical manifestations
2. src/voice/ module captures speech patterns, habits, and fingerprints
3. SubtextAnnotation has 1-3 emotion layers with tell signs
4. VoiceProfile differentiates characters by vocabulary, structure, habits
5. ToneStageEvaluator integrates subtext and voice checking
6. 'subtext-injection' and 'voice-drift' directives generated when issues found
7. JSON Schemas validate annotation and profile structures
8. All tests pass (25+ test cases expected across both modules)
</success_criteria>

<output>
After completion, create `.planning/phases/04-advanced-quality/04-03-SUMMARY.md`
</output>
